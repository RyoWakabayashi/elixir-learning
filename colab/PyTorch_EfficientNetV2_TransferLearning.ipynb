{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"rcilUdmZHbIx"},"source":["# 衛星データの画像分類\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RyoWakabayashi/elixir-learning/blob/main/colab/PyTorch_EfficientNetV2_TransferLearning.ipynb)\n","\n","衛星データ（可視光による衛星写真）を以下の種類に分類する\n","\n","- cloudy: くもり\n","- desert: 砂漠\n","- green_area: 緑地\n","- water: 水"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## ONNX のインストール"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install onnx"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B0QniaAygWI5"},"source":["## モジュールのインポート"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8HM_ohR0OPu"},"outputs":[],"source":["import copy\n","import math\n","import os\n","import random\n","import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import shutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","\n","from glob import glob\n","from google.colab import drive\n","\n","from torchvision import datasets, models, transforms"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0hhmURZOgadx"},"source":["## データの準備\n","\n","Google Drive 上の space/satellite.zip に Kaggle のデータを配置しておく\n","\n","https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XMYA3Cds-rl"},"outputs":[],"source":["drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWjyDKPHtOBW"},"outputs":[],"source":["!cp /content/drive/MyDrive/space/satellite.zip .\n","!unzip satellite.zip"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xf7lHU0gGyyA"},"source":["クラス名一覧を取得する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svIGVnyktjJz"},"outputs":[],"source":["classes = [os.path.basename(directory) for directory in sorted(glob(\"./data/*\"))]\n","\n","classes"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ezwt86GoGxrM"},"source":["テストデータを各クラスから25枚取得する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d7jy-ArvWDI"},"outputs":[],"source":["os.makedirs(\"./test_data\")\n","\n","for class_name in classes:\n","  os.makedirs(f\"./test_data/{class_name}\")\n","  target_files = random.sample(glob(f\"./data/{class_name}/*.jpg\"), 25)\n","  for target_file in target_files:\n","    basename = os.path.basename(target_file)\n","    shutil.move(target_file, f\"./test_data/{class_name}/{basename}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hICF-mx-G5dp"},"source":["残りのデータから各クラス8割をトレーニングデータとして取得する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWRmLCb_xUJb"},"outputs":[],"source":["os.makedirs(\"./train_data\")\n","\n","for class_name in classes:\n","  os.makedirs(f\"./train_data/{class_name}\")\n","  all_files = glob(f\"./data/{class_name}/*.jpg\")\n","  num_of_targets = math.floor(len(all_files) * 0.8)\n","  target_files = random.sample(all_files, num_of_targets)\n","  for target_file in target_files:\n","    basename = os.path.basename(target_file)\n","    shutil.move(target_file, f\"./train_data/{class_name}/{basename}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AnYtELA0HAhO"},"source":["残りを評価データにする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FsCFDcqx8OS"},"outputs":[],"source":["shutil.move(\"./data\", \"./val_data\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EKsMYMEXiHt4"},"source":["## モデル定義"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kwSbH-B3HH-L"},"source":["EfficientNet V2 の ImageNet データセット学習済モデルを転移学習元とする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeFnpZ1ZrDo_"},"outputs":[],"source":["model = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVuuAJFgrFFJ"},"outputs":[],"source":["for param in model.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a17AsrT40qa3"},"outputs":[],"source":["model.classifier"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ahWHnaD_HU2e"},"source":["出力層を4種類のクラス分類用に変更する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RlH3TvVrIsB"},"outputs":[],"source":["num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, len(classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7JOIuyE0x_c"},"outputs":[],"source":["model.classifier"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TIOK7LUNiRjx"},"source":["## データロード定義"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"obWvPisiIDMQ"},"source":["データの読み込み方を定義する\n","\n","- EfficientNet V2 M の入力層に合わせた画像サイズに変換する\n","- トレーニングデータは水平・垂直反転、回転・移動・拡大・縮小、台形変換、明度・コントラスト・彩度変化をランダムに発生させる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVDZt2NRrcgY"},"outputs":[],"source":["data_transforms = {\n","    \"train\": transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.RandomHorizontalFlip(0.5),\n","        transforms.RandomVerticalFlip(0.5),\n","        transforms.RandomAffine(degrees=[-10, 10], translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","        transforms.RandomPerspective(distortion_scale=0.1, p=0.9),\n","        transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor()\n","    ]),\n","    \"val\": transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor()\n","    ]),\n","    \"test\": transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor()\n","    ]),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOSShweN1kXP"},"outputs":[],"source":["data_kind = [\"train\", \"val\", \"test\"]\n","\n","image_datasets = {x: datasets.ImageFolder(f\"{x}_data\", data_transforms[x])\n","                  for x in data_kind}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=(x != \"test\"), num_workers=2)\n","              for x in data_kind}\n","dataset_sizes = {x: len(image_datasets[x]) for x in data_kind}\n","class_names = image_datasets[\"train\"].classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BukIoPoetTTv"},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WP4JaMw_IuxD"},"source":["ランダム変化の様子を確認する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBzMLnc41lvE"},"outputs":[],"source":["inputs, classes = next(iter(dataloaders['train']))\n","\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YcyiarYgoJcr"},"source":["## トレーニング定義"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xQfUakjIKo_5"},"source":["デバイスを取得する（GPU が使えれば GPU 、そうでなければ CPU）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EswApHiVKnGP"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y0jaScV3I0AP"},"source":["過学習を防ぐため、早期終了を定義する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-_-EN_H3iS7"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=5, verbose=0):\n","        self.epoch = 0\n","        self.pre_loss = float('inf')\n","        self.patience = patience\n","        self.verbose = verbose\n","\n","    def __call__(self, current_loss):\n","        if self.pre_loss < current_loss:\n","            self.epoch += 1\n","\n","            if self.epoch > self.patience:\n","                if self.verbose:\n","                    print('early stopping')\n","                return True\n","\n","        else:\n","            self.epoch = 0\n","            self.pre_loss = current_loss\n","\n","        return False"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6PyahlRGI5k8"},"source":["誤差、精度の推移をグラフとして保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKWF95zb_Ff0"},"outputs":[],"source":["def save_plots(value_dict, label):\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(\n","        value_dict['train'], color='blue', linestyle='-',\n","        label=f\"train {label}\"\n","    )\n","    plt.plot(\n","        value_dict['val'], color='orange', linestyle='-',\n","        label=f\"validataion {label}\"\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel(label.capitalize())\n","    plt.legend()\n","    plt.savefig(f\"./{label}.png\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qnYhw52cJCOa"},"source":["トレーニングを定義する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhEMVh-DrcVc"},"outputs":[],"source":["def train_model(model, criterion, optimizer, early_stopping, num_epochs=25):\n","    since = time.time()\n","\n","    loss_dict = {'train': [], 'val': []}\n","    acc_dict = {'train': [], 'val': []}\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    stop = False\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            loss_dict[phase].append(epoch_loss)\n","            acc_dict[phase].append(epoch_acc.to('cpu').detach().numpy().copy())\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            if phase == 'val':\n","                if early_stopping(epoch_loss):\n","                    stop = True\n","\n","                if epoch_acc > best_acc:\n","                    best_acc = epoch_acc\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        if stop:\n","            break\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, loss_dict, acc_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cwxi1UVJoPCt"},"source":["## トレーニング実行"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRx_9rfRtwRz"},"outputs":[],"source":["model = model.to(device)\n","\n","# 損失関数\n","criterion = nn.CrossEntropyLoss()\n","\n","# 最適化関数\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","\n","# 早期終了\n","early_stopping = EarlyStopping(patience=5, verbose=1)\n","\n","# トレーニング実行\n","model, loss_dict, acc_dict = train_model(model, criterion, optimizer, early_stopping, num_epochs=25)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1c-JBHGtJcGH"},"source":["モデルを保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4E9flzOjAg3Z"},"outputs":[],"source":["torch.save(model.state_dict(), \"./efficientnet_v2_m.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aFsw1n-YJepo"},"source":["誤差推移のグラフを保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7HBcOC5Axt5"},"outputs":[],"source":["save_plots(loss_dict, \"loss\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"V7M5b-hSJhGU"},"source":["精度推移のグラフを保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVENMCeuuLEj"},"outputs":[],"source":["save_plots(acc_dict, \"accuracy\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Us2nTYAIJzKK"},"source":["各出力を Google Drive に保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6-ELbH16qn8"},"outputs":[],"source":["!mkdir \"./drive/MyDrive/space/model\"\n","!cp \"./efficientnet_v2_m.pth\" \"./drive/MyDrive/space/model/\"\n","!cp \"./loss.png\" \"./drive/MyDrive/space/model/\"\n","!cp \"./accuracy.png\" \"./drive/MyDrive/space/model/\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2r5ZXOjkJ3m4"},"source":["テストデータを Google Drive に保存する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMxSK6pYuqTp"},"outputs":[],"source":["!zip -r \"test_data.zip\" \"test_data\"\n","!cp \"test_data.zip\" \"./drive/MyDrive/space/\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"R8Mu7vBmvxLl"},"source":["## 精度検証"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIC2dAtMVdU6"},"outputs":[],"source":["def visualize_model(model, dataset, num_images=12):\n","    model.eval()\n","    images_so_far = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders[dataset]):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                if labels[j] == preds[j]:\n","                    correct += 1\n","                images_so_far += 1\n","                print(f\"correct: {class_names[labels[j]]}\")\n","                print(f\"predicted: {class_names[preds[j]]}\")\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    return correct / num_images"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aGR39oodJ8J9"},"source":["評価データに対する精度"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HBYP4DiviUH"},"outputs":[],"source":["visualize_model(model, \"val\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SoNjvAiKKEiL"},"source":["保存したモデルをロードする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXPzb4u8Se2j"},"outputs":[],"source":["saved_model = models.efficientnet_v2_m()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jALokFxMS545"},"outputs":[],"source":["num_ftrs = saved_model.classifier[1].in_features\n","saved_model.classifier[1] = nn.Linear(num_ftrs, len(classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5hi4lCQC_pO"},"outputs":[],"source":["saved_model.load_state_dict(torch.load(\"efficientnet_v2_m.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8fSvv0LTcie"},"outputs":[],"source":["saved_model = saved_model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DolDGdMsJ-7I"},"source":["テストデータに対する精度"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSYqBf4BDGb4"},"outputs":[],"source":["visualize_model(saved_model, \"test\", 100)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## ONNX 形式への変換"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["onnx_file = \"efficientnet_v2_m.onnx\"\n","\n","dummy_img = torch.zeros(1, 3, 224, 224)\n","\n","cpu_device = torch.device(\"cpu\")\n","\n","cpu_model = saved_model.to(cpu_device)\n","dummy_img.to(cpu_device)\n","\n","torch.onnx.export(cpu_model, dummy_img, onnx_file, verbose=False, opset_version=12, input_names=['images'],\n","                  output_names=['predictions'],\n","                  dynamic_axes={'images': {0: 'batch_size'},})\n","\n","onnx_model = onnx.load(onnx_file)\n","onnx.checker.check_model(onnx_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cp \"./efficientnet_v2_m.onnx\" \"./drive/MyDrive/space/model/\""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMWBvmrYnt8eKQdNbd9UILd","gpuType":"T4","provenance":[{"file_id":"1A3U0AgtG4gg7OlPRzFXG93qep9dL0BXr","timestamp":1686960633135}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
